{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import common\n",
    "import util\n",
    "import ml_util\n",
    "importlib.reload(common)\n",
    "importlib.reload(util)\n",
    "importlib.reload(ml_util)\n",
    "\n",
    "from common import create_engine\n",
    "from common import display_all\n",
    "from common import figsize\n",
    "from common import save_df, load_df\n",
    "from common import save_session, load_session\n",
    "\n",
    "from util import show_importances\n",
    "from util import split_X_y_all, split_X_y, split_data\n",
    "from util import empty_features, column_feature, str_contains\n",
    "\n",
    "from ml_util import SelectFromModelPandas\n",
    "\n",
    "from pbar import Pbar\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters() # converters e.g. for datetime in plots\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 123\n",
    "np_random = np.random.RandomState(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakePredictor:\n",
    "    \n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return pd.to_numeric(self.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df('final_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>perex</th>\n",
       "      <th>body</th>\n",
       "      <th>raw_body</th>\n",
       "      <th>published_at</th>\n",
       "      <th>extracted_at</th>\n",
       "      <th>category</th>\n",
       "      <th>other_info</th>\n",
       "      <th>image_count</th>\n",
       "      <th>video_count</th>\n",
       "      <th>...</th>\n",
       "      <th>fb_popularity_ad_2</th>\n",
       "      <th>fb_popularity_ad_3</th>\n",
       "      <th>fb_popularity_ad_4</th>\n",
       "      <th>fb_popularity_ad_5</th>\n",
       "      <th>fb_popularity_ad_6</th>\n",
       "      <th>fb_popularity_ad_7</th>\n",
       "      <th>fb_popularity_ad_8</th>\n",
       "      <th>fb_popularity_ad_9</th>\n",
       "      <th>fb_popularity_ad_10</th>\n",
       "      <th>body_urls</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431065</th>\n",
       "      <td>put communities at the center of universal hea...</td>\n",
       "      <td>if universal health care is truly meant to ref...</td>\n",
       "      <td>if universal health care is truly meant to ref...</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"https://www.who.int/news-room/...</td>\n",
       "      <td>2019-10-21 10:45:10</td>\n",
       "      <td>2019-10-21 12:13:53.281652</td>\n",
       "      <td>[First Opinion]</td>\n",
       "      <td>{'tags': ['public health', 'global health', 'H...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>[https://www.who.int/news-room/detail/23-09-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431066</th>\n",
       "      <td>rapid expansion of telehealth comes with new c...</td>\n",
       "      <td>although new delivery methods will help telehe...</td>\n",
       "      <td>although new delivery methods will help telehe...</td>\n",
       "      <td>&lt;p&gt;It&amp;#x2019;s a boom time for telehealth. Sta...</td>\n",
       "      <td>2019-10-21 10:40:26</td>\n",
       "      <td>2019-10-21 12:13:53.499347</td>\n",
       "      <td>[First Opinion]</td>\n",
       "      <td>{'tags': ['telehealth'], 'keywords': ['']}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>[https://www.statnews.com/2019/10/21/telehealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431068</th>\n",
       "      <td>after decades-long campaign, type 3 poliovirus...</td>\n",
       "      <td>the formal bid to eradicate all polio began in...</td>\n",
       "      <td>the formal bid to eradicate all polio began in...</td>\n",
       "      <td>&lt;p&gt;After &lt;a href=\"https://www.statnews.com/201...</td>\n",
       "      <td>2019-10-21 10:30:40</td>\n",
       "      <td>2019-10-21 12:13:53.714328</td>\n",
       "      <td>[Health]</td>\n",
       "      <td>{'tags': ['public health', 'infectious disease...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>617.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>1137.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>[https://www.statnews.com/2019/10/21/decades-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431081</th>\n",
       "      <td>be humble, and proudly, psychologists say</td>\n",
       "      <td>humility is not the boldest of personality tra...</td>\n",
       "      <td>humility is not the boldest of personality tra...</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-21 00:00:00</td>\n",
       "      <td>2019-10-21 12:14:05.770730</td>\n",
       "      <td>None</td>\n",
       "      <td>{'tags': [], 'keywords': ['']}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3714.0</td>\n",
       "      <td>4217.0</td>\n",
       "      <td>5480.0</td>\n",
       "      <td>8674.0</td>\n",
       "      <td>9476.0</td>\n",
       "      <td>9867.0</td>\n",
       "      <td>10241.0</td>\n",
       "      <td>10792.0</td>\n",
       "      <td>11391.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431082</th>\n",
       "      <td>when teen drinking becomes a disorder</td>\n",
       "      <td>why are some adolescents more vulnerable than ...</td>\n",
       "      <td>why are some adolescents more vulnerable than ...</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-21 00:00:00</td>\n",
       "      <td>2019-10-21 12:14:05.836162</td>\n",
       "      <td>None</td>\n",
       "      <td>{'tags': [], 'keywords': ['']}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>172.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "id                                                          \n",
       "431065  put communities at the center of universal hea...   \n",
       "431066  rapid expansion of telehealth comes with new c...   \n",
       "431068  after decades-long campaign, type 3 poliovirus...   \n",
       "431081          be humble, and proudly, psychologists say   \n",
       "431082              when teen drinking becomes a disorder   \n",
       "\n",
       "                                                    perex  \\\n",
       "id                                                          \n",
       "431065  if universal health care is truly meant to ref...   \n",
       "431066  although new delivery methods will help telehe...   \n",
       "431068  the formal bid to eradicate all polio began in...   \n",
       "431081  humility is not the boldest of personality tra...   \n",
       "431082  why are some adolescents more vulnerable than ...   \n",
       "\n",
       "                                                     body  \\\n",
       "id                                                          \n",
       "431065  if universal health care is truly meant to ref...   \n",
       "431066  although new delivery methods will help telehe...   \n",
       "431068  the formal bid to eradicate all polio began in...   \n",
       "431081  humility is not the boldest of personality tra...   \n",
       "431082  why are some adolescents more vulnerable than ...   \n",
       "\n",
       "                                                 raw_body        published_at  \\\n",
       "id                                                                              \n",
       "431065  <p>The <a href=\"https://www.who.int/news-room/... 2019-10-21 10:45:10   \n",
       "431066  <p>It&#x2019;s a boom time for telehealth. Sta... 2019-10-21 10:40:26   \n",
       "431068  <p>After <a href=\"https://www.statnews.com/201... 2019-10-21 10:30:40   \n",
       "431081                                                    2019-10-21 00:00:00   \n",
       "431082                                                    2019-10-21 00:00:00   \n",
       "\n",
       "                     extracted_at         category  \\\n",
       "id                                                   \n",
       "431065 2019-10-21 12:13:53.281652  [First Opinion]   \n",
       "431066 2019-10-21 12:13:53.499347  [First Opinion]   \n",
       "431068 2019-10-21 12:13:53.714328         [Health]   \n",
       "431081 2019-10-21 12:14:05.770730             None   \n",
       "431082 2019-10-21 12:14:05.836162             None   \n",
       "\n",
       "                                               other_info  image_count  \\\n",
       "id                                                                       \n",
       "431065  {'tags': ['public health', 'global health', 'H...            1   \n",
       "431066         {'tags': ['telehealth'], 'keywords': ['']}            1   \n",
       "431068  {'tags': ['public health', 'infectious disease...            1   \n",
       "431081                     {'tags': [], 'keywords': ['']}            1   \n",
       "431082                     {'tags': [], 'keywords': ['']}            1   \n",
       "\n",
       "        video_count  ... fb_popularity_ad_2  fb_popularity_ad_3  \\\n",
       "id                   ...                                          \n",
       "431065            0  ...              165.0               176.0   \n",
       "431066            0  ...               44.0                47.0   \n",
       "431068            0  ...              617.0               673.0   \n",
       "431081            0  ...             3714.0              4217.0   \n",
       "431082            0  ...              172.0               198.0   \n",
       "\n",
       "       fb_popularity_ad_4 fb_popularity_ad_5 fb_popularity_ad_6  \\\n",
       "id                                                                \n",
       "431065              185.0              192.0              193.0   \n",
       "431066               47.0               47.0               49.0   \n",
       "431068              698.0              705.0              709.0   \n",
       "431081             5480.0             8674.0             9476.0   \n",
       "431082              218.0              229.0              243.0   \n",
       "\n",
       "        fb_popularity_ad_7 fb_popularity_ad_8  fb_popularity_ad_9  \\\n",
       "id                                                                  \n",
       "431065               207.0              210.0               228.0   \n",
       "431066                55.0               56.0                62.0   \n",
       "431068               913.0             1137.0              1197.0   \n",
       "431081              9867.0            10241.0             10792.0   \n",
       "431082               253.0              273.0               290.0   \n",
       "\n",
       "        fb_popularity_ad_10                                          body_urls  \n",
       "id                                                                              \n",
       "431065                233.0  [https://www.who.int/news-room/detail/23-09-20...  \n",
       "431066                 67.0  [https://www.statnews.com/2019/10/21/telehealt...  \n",
       "431068               1232.0  [https://www.statnews.com/2019/10/21/decades-l...  \n",
       "431081              11391.0                                                 []  \n",
       "431082                303.0                                                 []  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15458 entries, 431065 to 812426\n",
      "Data columns (total 68 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   title                    15458 non-null  object        \n",
      " 1   perex                    15458 non-null  object        \n",
      " 2   body                     15458 non-null  object        \n",
      " 3   raw_body                 15458 non-null  object        \n",
      " 4   published_at             15458 non-null  datetime64[ns]\n",
      " 5   extracted_at             15458 non-null  datetime64[ns]\n",
      " 6   category                 10307 non-null  object        \n",
      " 7   other_info               15458 non-null  object        \n",
      " 8   image_count              15458 non-null  int64         \n",
      " 9   video_count              15458 non-null  int64         \n",
      " 10  author_name              15458 non-null  object        \n",
      " 11  source_id                15458 non-null  int64         \n",
      " 12  source_name              15458 non-null  object        \n",
      " 13  source_url               15458 non-null  object        \n",
      " 14  source_type              15458 non-null  object        \n",
      " 15  source_is_reliable       15458 non-null  int64         \n",
      " 16  av_veracity              3533 non-null   object        \n",
      " 17  av_claims_false          15458 non-null  int64         \n",
      " 18  av_claims_mostly_false   15458 non-null  int64         \n",
      " 19  av_claims_mixture        15458 non-null  int64         \n",
      " 20  av_claims_mostly_true    15458 non-null  int64         \n",
      " 21  av_claims_true           15458 non-null  int64         \n",
      " 22  av_claims_unknown        15458 non-null  int64         \n",
      " 23  fb_ad_0_comment_count    15458 non-null  float64       \n",
      " 24  fb_ad_1_comment_count    15458 non-null  float64       \n",
      " 25  fb_ad_2_comment_count    15458 non-null  float64       \n",
      " 26  fb_ad_3_comment_count    15458 non-null  float64       \n",
      " 27  fb_ad_4_comment_count    15458 non-null  float64       \n",
      " 28  fb_ad_5_comment_count    15458 non-null  float64       \n",
      " 29  fb_ad_6_comment_count    15458 non-null  float64       \n",
      " 30  fb_ad_7_comment_count    15458 non-null  float64       \n",
      " 31  fb_ad_8_comment_count    15458 non-null  float64       \n",
      " 32  fb_ad_9_comment_count    15458 non-null  float64       \n",
      " 33  fb_ad_10_comment_count   15458 non-null  float64       \n",
      " 34  fb_ad_0_reaction_count   15458 non-null  float64       \n",
      " 35  fb_ad_1_reaction_count   15458 non-null  float64       \n",
      " 36  fb_ad_2_reaction_count   15458 non-null  float64       \n",
      " 37  fb_ad_3_reaction_count   15458 non-null  float64       \n",
      " 38  fb_ad_4_reaction_count   15458 non-null  float64       \n",
      " 39  fb_ad_5_reaction_count   15458 non-null  float64       \n",
      " 40  fb_ad_6_reaction_count   15458 non-null  float64       \n",
      " 41  fb_ad_7_reaction_count   15458 non-null  float64       \n",
      " 42  fb_ad_8_reaction_count   15458 non-null  float64       \n",
      " 43  fb_ad_9_reaction_count   15458 non-null  float64       \n",
      " 44  fb_ad_10_reaction_count  15458 non-null  float64       \n",
      " 45  fb_ad_0_share_count      15458 non-null  float64       \n",
      " 46  fb_ad_1_share_count      15458 non-null  float64       \n",
      " 47  fb_ad_2_share_count      15458 non-null  float64       \n",
      " 48  fb_ad_3_share_count      15458 non-null  float64       \n",
      " 49  fb_ad_4_share_count      15458 non-null  float64       \n",
      " 50  fb_ad_5_share_count      15458 non-null  float64       \n",
      " 51  fb_ad_6_share_count      15458 non-null  float64       \n",
      " 52  fb_ad_7_share_count      15458 non-null  float64       \n",
      " 53  fb_ad_8_share_count      15458 non-null  float64       \n",
      " 54  fb_ad_9_share_count      15458 non-null  float64       \n",
      " 55  fb_ad_10_share_count     15458 non-null  float64       \n",
      " 56  fb_popularity_ad_0       15458 non-null  float64       \n",
      " 57  fb_popularity_ad_1       15458 non-null  float64       \n",
      " 58  fb_popularity_ad_2       15458 non-null  float64       \n",
      " 59  fb_popularity_ad_3       15458 non-null  float64       \n",
      " 60  fb_popularity_ad_4       15458 non-null  float64       \n",
      " 61  fb_popularity_ad_5       15458 non-null  float64       \n",
      " 62  fb_popularity_ad_6       15458 non-null  float64       \n",
      " 63  fb_popularity_ad_7       15458 non-null  float64       \n",
      " 64  fb_popularity_ad_8       15458 non-null  float64       \n",
      " 65  fb_popularity_ad_9       15458 non-null  float64       \n",
      " 66  fb_popularity_ad_10      15458 non-null  float64       \n",
      " 67  body_urls                15458 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(44), int64(10), object(12)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15458"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>fb_popularity_ad_0</th>\n",
       "      <th>fb_popularity_ad_1</th>\n",
       "      <th>fb_popularity_ad_2</th>\n",
       "      <th>fb_popularity_ad_3</th>\n",
       "      <th>fb_popularity_ad_4</th>\n",
       "      <th>fb_popularity_ad_5</th>\n",
       "      <th>fb_popularity_ad_6</th>\n",
       "      <th>fb_popularity_ad_7</th>\n",
       "      <th>fb_popularity_ad_8</th>\n",
       "      <th>fb_popularity_ad_9</th>\n",
       "      <th>fb_popularity_ad_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>81.00</td>\n",
       "      <td>83.50</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>28.0</td>\n",
       "      <td>66.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>101.00</td>\n",
       "      <td>109.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>128.35</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>41.0</td>\n",
       "      <td>102.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>169.20</td>\n",
       "      <td>177.00</td>\n",
       "      <td>183.00</td>\n",
       "      <td>189.00</td>\n",
       "      <td>190.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>63.0</td>\n",
       "      <td>156.00</td>\n",
       "      <td>206.00</td>\n",
       "      <td>234.00</td>\n",
       "      <td>253.05</td>\n",
       "      <td>267.05</td>\n",
       "      <td>276.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>291.00</td>\n",
       "      <td>297.05</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>95.0</td>\n",
       "      <td>230.00</td>\n",
       "      <td>310.00</td>\n",
       "      <td>353.00</td>\n",
       "      <td>381.00</td>\n",
       "      <td>403.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>438.90</td>\n",
       "      <td>447.00</td>\n",
       "      <td>453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>145.0</td>\n",
       "      <td>351.00</td>\n",
       "      <td>468.75</td>\n",
       "      <td>532.00</td>\n",
       "      <td>578.00</td>\n",
       "      <td>606.00</td>\n",
       "      <td>629.75</td>\n",
       "      <td>651.00</td>\n",
       "      <td>663.75</td>\n",
       "      <td>674.00</td>\n",
       "      <td>685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>232.0</td>\n",
       "      <td>557.00</td>\n",
       "      <td>728.00</td>\n",
       "      <td>823.00</td>\n",
       "      <td>885.60</td>\n",
       "      <td>931.00</td>\n",
       "      <td>955.60</td>\n",
       "      <td>987.00</td>\n",
       "      <td>1003.00</td>\n",
       "      <td>1016.60</td>\n",
       "      <td>1036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>397.0</td>\n",
       "      <td>894.00</td>\n",
       "      <td>1174.00</td>\n",
       "      <td>1321.45</td>\n",
       "      <td>1435.45</td>\n",
       "      <td>1516.00</td>\n",
       "      <td>1570.90</td>\n",
       "      <td>1626.35</td>\n",
       "      <td>1657.45</td>\n",
       "      <td>1672.45</td>\n",
       "      <td>1707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>748.0</td>\n",
       "      <td>1678.70</td>\n",
       "      <td>2254.90</td>\n",
       "      <td>2501.10</td>\n",
       "      <td>2719.00</td>\n",
       "      <td>2884.30</td>\n",
       "      <td>3008.30</td>\n",
       "      <td>3115.90</td>\n",
       "      <td>3183.20</td>\n",
       "      <td>3219.30</td>\n",
       "      <td>3312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1909.6</td>\n",
       "      <td>4264.45</td>\n",
       "      <td>5579.05</td>\n",
       "      <td>6395.30</td>\n",
       "      <td>7168.10</td>\n",
       "      <td>7707.15</td>\n",
       "      <td>8109.60</td>\n",
       "      <td>8393.10</td>\n",
       "      <td>8673.20</td>\n",
       "      <td>8872.90</td>\n",
       "      <td>9126.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       q  fb_popularity_ad_0  fb_popularity_ad_1  fb_popularity_ad_2  \\\n",
       "0   0.00                 0.0                0.00                0.00   \n",
       "1   0.05                 0.0                0.00                0.00   \n",
       "2   0.10                 0.0                0.00                0.00   \n",
       "3   0.15                 0.0                0.00                0.00   \n",
       "4   0.20                 0.0                0.00                0.00   \n",
       "5   0.25                 1.0                1.00                1.00   \n",
       "6   0.30                 2.0                3.00                4.00   \n",
       "7   0.35                 4.0                7.00               11.00   \n",
       "8   0.40                 7.0               16.00               23.00   \n",
       "9   0.45                12.0               27.00               36.00   \n",
       "10  0.50                18.0               42.00               57.00   \n",
       "11  0.55                28.0               66.00               89.00   \n",
       "12  0.60                41.0              102.00              137.00   \n",
       "13  0.65                63.0              156.00              206.00   \n",
       "14  0.70                95.0              230.00              310.00   \n",
       "15  0.75               145.0              351.00              468.75   \n",
       "16  0.80               232.0              557.00              728.00   \n",
       "17  0.85               397.0              894.00             1174.00   \n",
       "18  0.90               748.0             1678.70             2254.90   \n",
       "19  0.95              1909.6             4264.45             5579.05   \n",
       "\n",
       "    fb_popularity_ad_3  fb_popularity_ad_4  fb_popularity_ad_5  \\\n",
       "0                 0.00                0.00                0.00   \n",
       "1                 0.00                0.00                0.00   \n",
       "2                 0.00                0.00                0.00   \n",
       "3                 0.00                0.00                0.00   \n",
       "4                 0.00                0.00                0.00   \n",
       "5                 1.00                2.00                2.00   \n",
       "6                 4.00                5.00                5.00   \n",
       "7                13.00               14.00               15.00   \n",
       "8                25.00               27.00               29.00   \n",
       "9                42.00               45.00               47.00   \n",
       "10               65.00               70.00               74.00   \n",
       "11              101.00              109.00              114.00   \n",
       "12              156.00              169.20              177.00   \n",
       "13              234.00              253.05              267.05   \n",
       "14              353.00              381.00              403.00   \n",
       "15              532.00              578.00              606.00   \n",
       "16              823.00              885.60              931.00   \n",
       "17             1321.45             1435.45             1516.00   \n",
       "18             2501.10             2719.00             2884.30   \n",
       "19             6395.30             7168.10             7707.15   \n",
       "\n",
       "    fb_popularity_ad_6  fb_popularity_ad_7  fb_popularity_ad_8  \\\n",
       "0                 0.00                0.00                0.00   \n",
       "1                 0.00                0.00                0.00   \n",
       "2                 0.00                0.00                0.00   \n",
       "3                 0.00                0.00                0.00   \n",
       "4                 0.00                0.00                0.00   \n",
       "5                 2.00                2.00                2.00   \n",
       "6                 6.00                6.00                6.00   \n",
       "7                17.00               17.00               17.00   \n",
       "8                31.00               32.00               32.00   \n",
       "9                49.00               50.00               51.00   \n",
       "10               77.00               79.00               81.00   \n",
       "11              118.00              123.00              125.00   \n",
       "12              183.00              189.00              190.00   \n",
       "13              276.00              284.00              291.00   \n",
       "14              414.00              430.00              438.90   \n",
       "15              629.75              651.00              663.75   \n",
       "16              955.60              987.00             1003.00   \n",
       "17             1570.90             1626.35             1657.45   \n",
       "18             3008.30             3115.90             3183.20   \n",
       "19             8109.60             8393.10             8673.20   \n",
       "\n",
       "    fb_popularity_ad_9  fb_popularity_ad_10  \n",
       "0                 0.00                  0.0  \n",
       "1                 0.00                  0.0  \n",
       "2                 0.00                  0.0  \n",
       "3                 0.00                  0.0  \n",
       "4                 0.00                  0.0  \n",
       "5                 2.00                  2.0  \n",
       "6                 6.00                  7.0  \n",
       "7                18.00                 19.0  \n",
       "8                33.00                 33.0  \n",
       "9                53.00                 53.0  \n",
       "10               83.50                 84.0  \n",
       "11              128.35                130.0  \n",
       "12              194.00                201.0  \n",
       "13              297.05                302.0  \n",
       "14              447.00                453.0  \n",
       "15              674.00                685.0  \n",
       "16             1016.60               1036.0  \n",
       "17             1672.45               1707.0  \n",
       "18             3219.30               3312.0  \n",
       "19             8872.90               9126.4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = pd.DataFrame()\n",
    "qrange = [np.round(i, 2) for i in np.arange(0, 1, 0.05)]\n",
    "pop['q'] = qrange\n",
    "for i in range(0, 11):\n",
    "    col = f'fb_popularity_ad_{i}'\n",
    "    pop[col] = [df[col].quantile(q) for q in qrange]\n",
    "pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozdelenie hodnot popularity do 4 skupin\n",
    "\n",
    "- `0 - 0.4`\n",
    "- `0.4 - 0.75`\n",
    "- `0.75 - 0.9`\n",
    "- `0.9 - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(df, quantiles, column='fb_popularity_ad_10'):\n",
    "    df = df.copy()\n",
    "    label_str = f'{column}_label'\n",
    "    \n",
    "    df[label_str] = -1\n",
    "    \n",
    "    label = 1    \n",
    "    for i in range(len(quantiles) - 1):\n",
    "        low = df[column].quantile(quantiles[i])\n",
    "        high = df[column].quantile(quantiles[i + 1])\n",
    "        \n",
    "        df.loc[(low <= df[column]) & (df[column] <= high), label_str] = int(label)\n",
    "        \n",
    "        label += 1\n",
    "    df = df.drop(columns=[column])    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00          0.00\n",
      "0.40         10.76\n",
      "0.75        386.00\n",
      "0.90       2008.24\n",
      "1.00    1368305.00\n",
      "Name: fb_ad_10_reaction_count, dtype: float64\n",
      "0.00         0.0\n",
      "0.40         1.0\n",
      "0.75        84.9\n",
      "0.90       513.3\n",
      "1.00    897945.0\n",
      "Name: fb_ad_10_comment_count, dtype: float64\n",
      "0.00         0.0\n",
      "0.40        17.0\n",
      "0.75       171.3\n",
      "0.90       676.0\n",
      "1.00    298199.0\n",
      "Name: fb_ad_10_share_count, dtype: float64\n",
      "0.00          0.0\n",
      "0.40         33.0\n",
      "0.75        685.0\n",
      "0.90       3312.0\n",
      "1.00    2564449.0\n",
      "Name: fb_popularity_ad_10, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quantiles = [\n",
    "    0,\n",
    "    .4,\n",
    "    .75,\n",
    "    .9,\n",
    "    1\n",
    "]\n",
    "\n",
    "cols = [\n",
    "    'fb_ad_10_reaction_count',\n",
    "    'fb_ad_10_comment_count',\n",
    "    'fb_ad_10_share_count',\n",
    "    'fb_popularity_ad_10'\n",
    "]\n",
    "\n",
    "for i in cols:\n",
    "    print(df[i].quantile(quantiles))\n",
    "    df = add_labels(df, quantiles, column=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6129\n",
       "2    5461\n",
       "3    2322\n",
       "4    1546\n",
       "Name: fb_popularity_ad_10_label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fb_popularity_ad_10_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri jednotlivych zlozkach sme pri tomto rozdeleni nasli len 4 skupiny (lebo 1 == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jednoducha heuristika: ak je zdroj nedoveryhodny tak aj clanok je nedoveryhodny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_fake_news_label'] = df.source_is_reliable.replace({0:1, 1:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(filter(lambda x: x.endswith('_label'), df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in label_names:\n",
    "    df[ln] = pd.to_numeric(df[ln])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labely\n",
    "labels_df = pd.concat([labels_df] + [df[label_name] for label_name in label_names], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozdelenie dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvorenie mnoziny clankov a zdrojov na ktorych sa validuje cela metoda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_data = df[df.source_name.isin([\n",
    "    'youngwomenshealth.org',\n",
    "    'vaxopedia.org',\n",
    "    'emedicinehealth.com',\n",
    "    \n",
    "    'hsionline.com',\n",
    "    'wakeup-world.com',\n",
    "    'genuinehealth.com',\n",
    "    'realfarmacy.com',\n",
    "    'educateinspirechange.org',\n",
    "    'vaxxter.com'\n",
    "])].copy()\n",
    "\n",
    "df = df[~df.source_name.isin(excluded_data.source_name.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emedicinehealth.com         230\n",
       "realfarmacy.com             182\n",
       "hsionline.com               171\n",
       "vaxopedia.org                78\n",
       "vaxxter.com                  38\n",
       "wakeup-world.com             38\n",
       "educateinspirechange.org     31\n",
       "youngwomenshealth.org        17\n",
       "genuinehealth.com            17\n",
       "Name: source_name, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_data.source_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    477\n",
       "1    325\n",
       "Name: source_is_reliable, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_data.source_is_reliable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tuple(split_data(df, sizes=[3, 1], shuffle=True, np_random=np_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', inplace=True)\n",
    "test.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_data = excluded_data.reset_index()\n",
    "excluded_data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(train, 'train_data')\n",
    "save_df(test, 'test_data')\n",
    "save_df(excluded_data, 'excluded_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10992, 3664, 802]\n"
     ]
    }
   ],
   "source": [
    "print([len(i) for i in [train,test, excluded_data]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fb_ad_10_reaction_count_label',\n",
       " 'fb_ad_10_comment_count_label',\n",
       " 'fb_ad_10_share_count_label',\n",
       " 'fb_popularity_ad_10_label',\n",
       " 'is_fake_news_label']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_X_y_all(train, test, excluded_data, 'is_fake_news_label', label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "importlib.reload(spacy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text, disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "    return words_from_doc(doc)\n",
    "\n",
    "def words_from_doc(doc):\n",
    "    res = []\n",
    "    for i in doc:\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.is_punct:\n",
    "            continue\n",
    "\n",
    "        res.append(str(i))\n",
    "\n",
    "    return res\n",
    "\n",
    "def tokenize_to_file(data, file):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for i in Pbar(data):\n",
    "            f.write(f\"{' '.join(tokenize(i))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening doc2vec modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] -- 10992 / 10992 -- (finished)\n",
      "[==================================================] -- 3664 / 3664 -- (finished)\n",
      "[==================================================] -- 802 / 802 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "tokenize_to_file(data.train.X.body, './data/train_body_tokenized.txt')\n",
    "tokenize_to_file(data.test.X.body, './data/test_body_tokenized.txt')\n",
    "tokenize_to_file(data.validation.X.body, './data/validation_data_body_tokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 18:51:22,256 : INFO : collecting all words and their counts\n",
      "2020-05-06 18:51:22,257 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-05-06 18:51:23,474 : INFO : PROGRESS: at example #10000, processed 4182706 words (3439909/s), 110285 word types, 10000 tags\n",
      "2020-05-06 18:51:23,592 : INFO : collected 116016 word types and 10992 unique tags from a corpus of 10992 examples and 4599596 words\n",
      "2020-05-06 18:51:23,593 : INFO : Loading a fresh vocabulary\n",
      "2020-05-06 18:51:23,774 : INFO : effective_min_count=2 retains 66229 unique words (57% of original 116016, drops 49787)\n",
      "2020-05-06 18:51:23,775 : INFO : effective_min_count=2 leaves 4549809 word corpus (98% of original 4599596, drops 49787)\n",
      "2020-05-06 18:51:23,997 : INFO : deleting the raw counts dictionary of 116016 items\n",
      "2020-05-06 18:51:24,000 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2020-05-06 18:51:24,001 : INFO : downsampling leaves estimated 4499511 word corpus (98.9% of prior 4549809)\n",
      "2020-05-06 18:51:24,185 : INFO : estimated required memory for 66229 words and 300 dimensions: 205254500 bytes\n",
      "2020-05-06 18:51:24,186 : INFO : resetting layer weights\n",
      "2020-05-06 18:51:40,810 : INFO : training model with 12 workers on 66229 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-06 18:51:44,191 : INFO : EPOCH 1 - PROGRESS: at 8.10% examples, 114520 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:51:44,193 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:51:44,269 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:51:44,303 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:51:44,307 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:51:44,364 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:51:44,386 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:51:44,404 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:51:44,495 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:51:44,521 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:51:44,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:51:44,566 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:51:44,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:51:44,755 : INFO : EPOCH - 1 : training on 4601151 raw words (4512325 effective words) took 3.8s, 1172255 effective words/s\n",
      "2020-05-06 18:51:48,066 : INFO : EPOCH 2 - PROGRESS: at 8.31% examples, 115842 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:51:48,068 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:51:48,081 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:51:48,154 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:51:48,206 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:51:48,262 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:51:48,329 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:51:48,338 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:51:48,351 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:51:48,358 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:51:48,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:51:48,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:51:48,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:51:48,455 : INFO : EPOCH - 2 : training on 4601151 raw words (4512235 effective words) took 3.6s, 1241826 effective words/s\n",
      "2020-05-06 18:51:51,778 : INFO : EPOCH 3 - PROGRESS: at 8.10% examples, 115477 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:51:51,779 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:51:51,801 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:51:51,807 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:51:51,823 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:51:51,831 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:51:51,901 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:51:51,949 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:51:51,997 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:51:52,039 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:51:52,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:51:52,059 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:51:52,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:51:52,139 : INFO : EPOCH - 3 : training on 4601151 raw words (4512116 effective words) took 3.6s, 1247216 effective words/s\n",
      "2020-05-06 18:51:55,460 : INFO : EPOCH 4 - PROGRESS: at 8.72% examples, 115711 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:51:55,461 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:51:55,462 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:51:55,506 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:51:55,510 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:51:55,523 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:51:55,533 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:51:55,638 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:51:55,660 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:51:55,699 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:51:55,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:51:55,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:51:55,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:51:55,774 : INFO : EPOCH - 4 : training on 4601151 raw words (4512053 effective words) took 3.6s, 1266195 effective words/s\n",
      "2020-05-06 18:51:59,096 : INFO : EPOCH 5 - PROGRESS: at 8.31% examples, 115711 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:51:59,097 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:51:59,128 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:51:59,139 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:51:59,155 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:51:59,165 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:51:59,173 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:51:59,184 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:51:59,242 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:51:59,337 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:51:59,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:51:59,349 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:51:59,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:51:59,360 : INFO : EPOCH - 5 : training on 4601151 raw words (4512699 effective words) took 3.5s, 1284196 effective words/s\n",
      "2020-05-06 18:52:02,739 : INFO : EPOCH 6 - PROGRESS: at 8.10% examples, 114757 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:02,741 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:02,751 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:02,767 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:02,783 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:02,792 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:02,794 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:02,825 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:02,842 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:02,918 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:02,961 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:02,995 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:03,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:03,005 : INFO : EPOCH - 6 : training on 4601151 raw words (4512240 effective words) took 3.5s, 1273031 effective words/s\n",
      "2020-05-06 18:52:06,289 : INFO : EPOCH 7 - PROGRESS: at 8.72% examples, 117808 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:06,290 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:06,325 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:06,351 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:06,370 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:06,372 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:06,394 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:06,428 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:06,496 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:06,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:06,643 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:06,664 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:06,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:06,666 : INFO : EPOCH - 7 : training on 4601151 raw words (4512057 effective words) took 3.6s, 1264336 effective words/s\n",
      "2020-05-06 18:52:09,948 : INFO : EPOCH 8 - PROGRESS: at 8.16% examples, 117123 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:09,949 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:09,995 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:10,032 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:10,047 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:10,059 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:10,089 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:10,111 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:10,130 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:10,203 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:10,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:10,261 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:10,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:10,265 : INFO : EPOCH - 8 : training on 4601151 raw words (4512051 effective words) took 3.5s, 1278707 effective words/s\n",
      "2020-05-06 18:52:13,541 : INFO : EPOCH 9 - PROGRESS: at 8.31% examples, 118375 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:13,543 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:13,565 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:13,647 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:13,672 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:13,733 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:13,747 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:13,770 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:13,785 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:13,787 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:13,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:13,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:13,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:13,836 : INFO : EPOCH - 9 : training on 4601151 raw words (4512466 effective words) took 3.5s, 1300363 effective words/s\n",
      "2020-05-06 18:52:17,251 : INFO : EPOCH 10 - PROGRESS: at 8.31% examples, 112615 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:17,252 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:17,257 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:17,293 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:17,358 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:17,365 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:17,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:17,379 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:17,406 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:17,469 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:17,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:17,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:17,582 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:17,583 : INFO : EPOCH - 10 : training on 4601151 raw words (4512054 effective words) took 3.7s, 1228290 effective words/s\n",
      "2020-05-06 18:52:20,786 : INFO : EPOCH 11 - PROGRESS: at 8.72% examples, 120102 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:20,787 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:20,839 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:20,851 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:20,868 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:20,880 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:20,890 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:20,928 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:20,958 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:21,156 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:21,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:21,224 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:21,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:21,255 : INFO : EPOCH - 11 : training on 4601151 raw words (4512045 effective words) took 3.6s, 1253332 effective words/s\n",
      "2020-05-06 18:52:24,467 : INFO : EPOCH 12 - PROGRESS: at 8.31% examples, 120620 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:24,469 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:24,484 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:24,503 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:24,504 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:24,517 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:24,523 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:24,591 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:24,631 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:24,795 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:24,815 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:24,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:24,899 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:24,900 : INFO : EPOCH - 12 : training on 4601151 raw words (4512412 effective words) took 3.5s, 1271273 effective words/s\n",
      "2020-05-06 18:52:28,135 : INFO : EPOCH 13 - PROGRESS: at 8.49% examples, 118876 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:28,137 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:28,156 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:28,157 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:28,166 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:28,207 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:28,212 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:28,252 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:28,313 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:28,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:28,443 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:28,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:28,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:28,484 : INFO : EPOCH - 13 : training on 4601151 raw words (4512186 effective words) took 3.5s, 1284263 effective words/s\n",
      "2020-05-06 18:52:31,721 : INFO : EPOCH 14 - PROGRESS: at 8.31% examples, 119842 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:31,722 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:31,759 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:31,766 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:31,787 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:31,801 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:31,880 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:31,888 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:31,924 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:31,994 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:32,026 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:32,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:32,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:32,147 : INFO : EPOCH - 14 : training on 4601151 raw words (4512300 effective words) took 3.6s, 1266651 effective words/s\n",
      "2020-05-06 18:52:35,322 : INFO : EPOCH 15 - PROGRESS: at 8.31% examples, 122368 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:35,323 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:35,397 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:35,430 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:35,454 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:35,456 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:35,462 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:35,530 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:35,537 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:35,634 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:35,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:35,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:35,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:35,735 : INFO : EPOCH - 15 : training on 4601151 raw words (4511781 effective words) took 3.5s, 1294729 effective words/s\n",
      "2020-05-06 18:52:38,938 : INFO : EPOCH 16 - PROGRESS: at 8.16% examples, 121166 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:38,940 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:38,959 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:38,993 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:38,998 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:39,019 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:39,037 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:39,090 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:39,145 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:39,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:39,250 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:39,265 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:39,289 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:39,290 : INFO : EPOCH - 16 : training on 4601151 raw words (4512251 effective words) took 3.5s, 1305611 effective words/s\n",
      "2020-05-06 18:52:42,526 : INFO : EPOCH 17 - PROGRESS: at 8.10% examples, 119029 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:42,528 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:42,532 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:42,558 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:42,564 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:42,573 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:42,576 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:42,674 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:42,748 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:42,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:42,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:42,819 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:42,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:42,856 : INFO : EPOCH - 17 : training on 4601151 raw words (4512051 effective words) took 3.5s, 1292613 effective words/s\n",
      "2020-05-06 18:52:46,067 : INFO : EPOCH 18 - PROGRESS: at 8.49% examples, 120773 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:46,069 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:46,092 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:46,094 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:46,151 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:46,163 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:46,193 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:46,232 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:46,244 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:46,248 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:46,251 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:46,304 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:46,325 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:46,326 : INFO : EPOCH - 18 : training on 4601151 raw words (4512200 effective words) took 3.4s, 1337178 effective words/s\n",
      "2020-05-06 18:52:49,534 : INFO : EPOCH 19 - PROGRESS: at 8.31% examples, 120930 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:49,536 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:49,539 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:49,548 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:49,558 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:49,592 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:49,638 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:49,650 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:49,706 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:49,733 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:49,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:49,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:49,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:49,834 : INFO : EPOCH - 19 : training on 4601151 raw words (4511915 effective words) took 3.4s, 1323657 effective words/s\n",
      "2020-05-06 18:52:53,010 : INFO : EPOCH 20 - PROGRESS: at 8.72% examples, 120973 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:53,011 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:53,024 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:53,045 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:53,052 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:53,080 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:53,188 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:53,205 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:53,261 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:53,284 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:53,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:53,330 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:53,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:53,449 : INFO : EPOCH - 20 : training on 4601151 raw words (4511963 effective words) took 3.5s, 1271716 effective words/s\n",
      "2020-05-06 18:52:56,598 : INFO : EPOCH 21 - PROGRESS: at 8.16% examples, 123374 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:52:56,599 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:52:56,626 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:52:56,641 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:52:56,669 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:52:56,723 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:52:56,767 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:52:56,866 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:52:56,888 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:52:56,934 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:52:56,945 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:52:56,946 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:52:57,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:52:57,012 : INFO : EPOCH - 21 : training on 4601151 raw words (4511870 effective words) took 3.5s, 1303208 effective words/s\n",
      "2020-05-06 18:53:00,213 : INFO : EPOCH 22 - PROGRESS: at 8.31% examples, 121402 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:53:00,215 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:53:00,219 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:53:00,287 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:53:00,358 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:53:00,374 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:53:00,413 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:53:00,433 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:53:00,456 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:53:00,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:53:00,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:53:00,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:53:00,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:53:00,731 : INFO : EPOCH - 22 : training on 4601151 raw words (4512036 effective words) took 3.6s, 1248856 effective words/s\n",
      "2020-05-06 18:53:04,017 : INFO : EPOCH 23 - PROGRESS: at 8.49% examples, 117518 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:53:04,018 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:53:04,023 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:53:04,054 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:53:04,059 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:53:04,111 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:53:04,235 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:53:04,242 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:53:04,250 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:53:04,322 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:53:04,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:53:04,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:53:04,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:53:04,417 : INFO : EPOCH - 23 : training on 4601151 raw words (4512078 effective words) took 3.6s, 1252932 effective words/s\n",
      "2020-05-06 18:53:07,623 : INFO : EPOCH 24 - PROGRESS: at 7.89% examples, 120085 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:53:07,625 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:53:07,646 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:53:07,673 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:53:07,677 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:53:07,687 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:53:07,692 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:53:07,714 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:53:07,842 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:53:07,932 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:53:07,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:53:08,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:53:08,031 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:53:08,032 : INFO : EPOCH - 24 : training on 4601151 raw words (4512369 effective words) took 3.5s, 1274152 effective words/s\n",
      "2020-05-06 18:53:11,226 : INFO : EPOCH 25 - PROGRESS: at 8.16% examples, 121122 words/s, in_qsize -1, out_qsize 1\n",
      "2020-05-06 18:53:11,227 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-06 18:53:11,237 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-06 18:53:11,267 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-06 18:53:11,273 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-06 18:53:11,298 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-06 18:53:11,337 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-06 18:53:11,368 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-06 18:53:11,418 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-06 18:53:11,632 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-06 18:53:11,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-06 18:53:11,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-06 18:53:11,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-06 18:53:11,732 : INFO : EPOCH - 25 : training on 4601151 raw words (4512007 effective words) took 3.6s, 1249257 effective words/s\n",
      "2020-05-06 18:53:11,738 : INFO : training on a 115028775 raw words (112803760 effective words) took 90.9s, 1240605 effective words/s\n"
     ]
    }
   ],
   "source": [
    "d2v = Doc2Vec(corpus_file='./data/train_body_tokenized.txt', vector_size=300, min_count=2, epochs=25, workers=12, seed=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_d2v(d2v_model, data_file):\n",
    "    res = []\n",
    "    \n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        for i in Pbar(f.readlines()):\n",
    "            res.append(d2v_model.infer_vector(i.split(' '), steps=20, alpha=0.025)) \n",
    "    \n",
    "    return res\n",
    "\n",
    "def infer_for_df(df, d2v_model, data_file):\n",
    "    lst = infer_d2v(d2v_model, data_file)\n",
    "    d2v_df = pd.DataFrame(lst, index=df.index, columns=[f'd2v_{i}' for i in range(1, 301)] )\n",
    "    \n",
    "    return d2v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nacitanie predpripravenych crt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sentiment = load_df('features_sentiment.pickle')\n",
    "f_readability = load_df('features_readability.pickle')\n",
    "f_metadata = load_df('features_metadata.pickle')\n",
    "f_empath = load_df('features_empath.pickle')\n",
    "f_content = load_df('features_content.pickle')\n",
    "f_named_entities = load_df('features_named_entities.pickle')\n",
    "f_popularity = load_df('features_popularity.pickle')\n",
    "f_popularity_d0 = load_df('features_popularity_0.pickle')\n",
    "f_popularity_d1 = load_df('features_popularity_1.pickle')\n",
    "f_popularity_d2 = load_df('features_popularity_2.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ziskanie d2v vektorov pre train a test body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] -- 10992 / 10992 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "f_d2v_train = infer_for_df(data.train.X, d2v, './data/train_body_tokenized.txt')\n",
    "save_df(f_d2v_train, 'd2v_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] -- 3664 / 3664 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "f_d2v_test = infer_for_df(data.test.X, d2v, './data/test_body_tokenized.txt')\n",
    "save_df(f_d2v_test, 'd2v_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] -- 802 / 802 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "f_d2v_validation = infer_for_df(data.validation.X, d2v, './data/validation_data_body_tokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vytvorenie jednotlivych feature setov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.features = pd.concat([\n",
    "    pd.DataFrame(index=data.train.X.index),\n",
    "    f_sentiment,\n",
    "    f_readability,\n",
    "    f_metadata,\n",
    "    f_empath,\n",
    "    f_content,\n",
    "    f_named_entities,\n",
    "    f_d2v_train,\n",
    "    f_popularity_d0,\n",
    "    f_popularity_d1,\n",
    "    f_popularity_d2\n",
    "], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.features = pd.concat([\n",
    "    pd.DataFrame(index=data.test.X.index),\n",
    "    f_sentiment,\n",
    "    f_readability,\n",
    "    f_metadata,\n",
    "    f_empath,\n",
    "    f_content,\n",
    "    f_named_entities,\n",
    "    f_d2v_test,\n",
    "    f_popularity_d0,\n",
    "    f_popularity_d1,\n",
    "    f_popularity_d2\n",
    "], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.validation.features = pd.concat([\n",
    "    pd.DataFrame(index=data.validation.X.index),\n",
    "    f_sentiment,\n",
    "    f_readability,\n",
    "    f_metadata,\n",
    "    f_empath,\n",
    "    f_content,\n",
    "    f_named_entities,\n",
    "    f_d2v_validation,\n",
    "    f_popularity_d0,\n",
    "    f_popularity_d1,\n",
    "    f_popularity_d2\n",
    "], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilitne metody pre jednoduchsiu pracu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "\n",
    "    def __init__(self, train, train_y, test, test_y, clfs=None):\n",
    "        if clfs is None:\n",
    "            self.classifiers = [\n",
    "                XGBClassifier(n_jobs=12, seed=RANDOM_STATE),\n",
    "                RandomForestClassifier(n_estimators=100, n_jobs=12, class_weight='balanced', random_state=RANDOM_STATE),\n",
    "                DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE)\n",
    "            ]\n",
    "        else:\n",
    "            self.classifiers = clfs\n",
    "            \n",
    "        \n",
    "        self.train = train\n",
    "        self.train_y = train_y\n",
    "        self.test = test\n",
    "        self.test_y = test_y\n",
    "        \n",
    "\n",
    "    def fit_predict(self, clf):\n",
    "        clf.fit(self.train, self.train_y)\n",
    "        return clf.predict(self.test)\n",
    "    \n",
    "    def run(self):\n",
    "        _pbar_conf = {\n",
    "            'refresh_rate': 1,\n",
    "            'pbar_width': 52,\n",
    "            'length': len(self.classifiers),\n",
    "            'action_names': [i.__class__.__name__ for i in self.classifiers]\n",
    "        }\n",
    "        \n",
    "        for p in Pbar((self.fit_predict(clf) for clf in self.classifiers), **_pbar_conf):\n",
    "            print(classification_report(self.test_y, p))\n",
    "            print(confusion_matrix(self.test_y, p))\n",
    "            print('-' * 54)\n",
    "            \n",
    "        return self.classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_source_features(df):\n",
    "    cols = [col for col in df if col.startswith('source_')]\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "def limit_features(df, day, source=False):\n",
    "    \n",
    "    if source is False:\n",
    "        df = drop_source_features(df)  \n",
    "    \n",
    "    for i in range(2, day-1, -1):\n",
    "        cols = [\n",
    "                f'fb_ad_{i}_reaction_count',\n",
    "                f'fb_ad_{i}_share_count',\n",
    "                f'fb_ad_{i}_comment_count',\n",
    "                f'fb_popularity_ad_{i}'\n",
    "        ]\n",
    "        df = df.drop(columns=cols)\n",
    "        \n",
    "    return df.copy()\n",
    "\n",
    "def popularity_features(df, day):\n",
    "    df = df.drop(columns=['popularity_prediction'], errors='ignore')\n",
    "    return limit_features(df, day=day, source=True)\n",
    "\n",
    "def detection_features(df, day, pop_predictor=None):\n",
    "    df = df.drop(columns=['popularity_prediction'], errors='ignore')\n",
    "    if pop_predictor is not None:\n",
    "        df['popularity_prediction'] = pop_predictor.predict(limit_features(df, day=day, source=True))\n",
    "\n",
    "    return limit_features(df, day=day, source=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pociatocne parametre pre random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizacia experimentov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situacia: Moment publikovania (ziadne data o popularite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nastavenie labelu\n",
    "data.train.switch_label('fb_popularity_ad_10_label')\n",
    "data.test.switch_label('fb_popularity_ad_10_label')\n",
    "data.validation.switch_label('fb_popularity_ad_10_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predikcia popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.78      0.84      1397\n",
      "           2       0.59      0.88      0.70      1312\n",
      "           3       0.42      0.10      0.16       575\n",
      "           4       0.51      0.49      0.50       380\n",
      "\n",
      "    accuracy                           0.68      3664\n",
      "   macro avg       0.61      0.56      0.55      3664\n",
      "weighted avg       0.68      0.68      0.65      3664\n",
      "\n",
      "[[1083  308    2    4]\n",
      " [  75 1157   29   51]\n",
      " [  20  374   57  124]\n",
      " [   8  138   47  187]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.82      0.74      1397\n",
      "           2       0.54      0.73      0.62      1312\n",
      "           3       0.30      0.06      0.10       575\n",
      "           4       0.52      0.11      0.19       380\n",
      "\n",
      "    accuracy                           0.59      3664\n",
      "   macro avg       0.51      0.43      0.41      3664\n",
      "weighted avg       0.55      0.59      0.54      3664\n",
      "\n",
      "[[1146  248    2    1]\n",
      " [ 315  957   30   10]\n",
      " [ 140  373   33   29]\n",
      " [  96  196   45   43]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.73      0.74      1397\n",
      "           2       0.53      0.53      0.53      1312\n",
      "           3       0.29      0.30      0.29       575\n",
      "           4       0.30      0.31      0.30       380\n",
      "\n",
      "    accuracy                           0.55      3664\n",
      "   macro avg       0.47      0.47      0.47      3664\n",
      "weighted avg       0.55      0.55      0.55      3664\n",
      "\n",
      "[[1013  271   66   47]\n",
      " [ 270  701  228  113]\n",
      " [  43  246  171  115]\n",
      " [  27  112  124  117]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_pp_clf = Runner(train=popularity_features(pd.concat([data.train.features, data.validation.features]), day=0),\n",
    "                train_y=pd.concat([data.train.y, data.validation.y]),\n",
    "                test=popularity_features(data.test.features, day=0),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detekcia falosnych sprav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.switch_label('is_fake_news_label')\n",
    "data.test.switch_label('is_fake_news_label')\n",
    "data.validation.switch_label('is_fake_news_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bez pouzitia predikcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2184\n",
      "           1       0.98      0.95      0.96      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2153   31]\n",
      " [  76 1404]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96      2184\n",
      "           1       0.98      0.88      0.93      1480\n",
      "\n",
      "    accuracy                           0.95      3664\n",
      "   macro avg       0.95      0.94      0.94      3664\n",
      "weighted avg       0.95      0.95      0.95      3664\n",
      "\n",
      "[[2163   21]\n",
      " [ 178 1302]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2184\n",
      "           1       0.89      0.87      0.88      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.90      0.90      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2030  154]\n",
      " [ 198 1282]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_fn_clf1 = Runner(train=detection_features(data.train.features, day=0),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=0),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s vyuzitim predikcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2184\n",
      "           1       0.98      0.95      0.96      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2150   34]\n",
      " [  69 1411]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      2184\n",
      "           1       0.99      0.89      0.94      1480\n",
      "\n",
      "    accuracy                           0.95      3664\n",
      "   macro avg       0.96      0.94      0.95      3664\n",
      "weighted avg       0.95      0.95      0.95      3664\n",
      "\n",
      "[[2169   15]\n",
      " [ 160 1320]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2184\n",
      "           1       0.89      0.86      0.87      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.89      0.89      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2021  163]\n",
      " [ 207 1273]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_fn_clf2 = Runner(train=detection_features(data.train.features, day=0, pop_predictor=ad0_pp_clf[0]),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=0, pop_predictor=ad0_pp_clf[0]),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validacna sada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validacia bez pouzitia predikcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.80       325\n",
      "           1       0.84      0.93      0.88       477\n",
      "\n",
      "    accuracy                           0.85       802\n",
      "   macro avg       0.86      0.83      0.84       802\n",
      "weighted avg       0.85      0.85      0.85       802\n",
      "\n",
      "[[239  86]\n",
      " [ 33 444]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       325\n",
      "           1       0.87      0.85      0.86       477\n",
      "\n",
      "    accuracy                           0.83       802\n",
      "   macro avg       0.83      0.83      0.83       802\n",
      "weighted avg       0.84      0.83      0.83       802\n",
      "\n",
      "[[263  62]\n",
      " [ 71 406]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       325\n",
      "           1       0.81      0.79      0.80       477\n",
      "\n",
      "    accuracy                           0.77       802\n",
      "   macro avg       0.76      0.76      0.76       802\n",
      "weighted avg       0.77      0.77      0.77       802\n",
      "\n",
      "[[238  87]\n",
      " [ 99 378]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_fn_valid_clf1 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=0),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=0),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validacia s pouzitim predikcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81       325\n",
      "           1       0.85      0.94      0.89       477\n",
      "\n",
      "    accuracy                           0.86       802\n",
      "   macro avg       0.87      0.84      0.85       802\n",
      "weighted avg       0.86      0.86      0.86       802\n",
      "\n",
      "[[243  82]\n",
      " [ 29 448]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       325\n",
      "           1       0.88      0.84      0.86       477\n",
      "\n",
      "    accuracy                           0.84       802\n",
      "   macro avg       0.83      0.84      0.83       802\n",
      "weighted avg       0.84      0.84      0.84       802\n",
      "\n",
      "[[272  53]\n",
      " [ 76 401]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       325\n",
      "           1       0.83      0.79      0.81       477\n",
      "\n",
      "    accuracy                           0.77       802\n",
      "   macro avg       0.77      0.77      0.77       802\n",
      "weighted avg       0.78      0.77      0.78       802\n",
      "\n",
      "[[246  79]\n",
      " [102 375]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_fn_valid_clf2 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=0, pop_predictor=ad0_pp_clf[0]),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=0, pop_predictor=ad0_pp_clf[0]),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validacia s realnymi datami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81       325\n",
      "           1       0.84      0.94      0.89       477\n",
      "\n",
      "    accuracy                           0.86       802\n",
      "   macro avg       0.87      0.84      0.85       802\n",
      "weighted avg       0.86      0.86      0.86       802\n",
      "\n",
      "[[241  84]\n",
      " [ 28 449]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       325\n",
      "           1       0.87      0.83      0.85       477\n",
      "\n",
      "    accuracy                           0.83       802\n",
      "   macro avg       0.82      0.83      0.82       802\n",
      "weighted avg       0.83      0.83      0.83       802\n",
      "\n",
      "[[267  58]\n",
      " [ 80 397]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       325\n",
      "           1       0.82      0.79      0.80       477\n",
      "\n",
      "    accuracy                           0.77       802\n",
      "   macro avg       0.76      0.76      0.76       802\n",
      "weighted avg       0.77      0.77      0.77       802\n",
      "\n",
      "[[241  84]\n",
      " [101 376]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad0_fn_valid_clf3 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=0,\n",
    "                                                              pop_predictor=FakePredictor(pd.concat([data.train.y_all['fb_popularity_ad_10_label'], data.test.y_all['fb_popularity_ad_10_label']]))),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=0, pop_predictor=FakePredictor(data.validation.y_all['fb_popularity_ad_10_label'])),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situacia: Moment 1 den po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nastavenie labelu\n",
    "data.train.switch_label('fb_popularity_ad_10_label')\n",
    "data.test.switch_label('fb_popularity_ad_10_label')\n",
    "data.validation.switch_label('fb_popularity_ad_10_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.95      0.91      1397\n",
      "           2       0.81      0.83      0.82      1312\n",
      "           3       0.76      0.65      0.70       575\n",
      "           4       0.89      0.75      0.81       380\n",
      "\n",
      "    accuracy                           0.84      3664\n",
      "   macro avg       0.83      0.80      0.81      3664\n",
      "weighted avg       0.84      0.84      0.84      3664\n",
      "\n",
      "[[1324   70    0    3]\n",
      " [ 162 1092   51    7]\n",
      " [  19  156  373   27]\n",
      " [   2   28   64  286]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.92      0.89      1397\n",
      "           2       0.72      0.86      0.78      1312\n",
      "           3       0.69      0.41      0.52       575\n",
      "           4       0.91      0.67      0.77       380\n",
      "\n",
      "    accuracy                           0.79      3664\n",
      "   macro avg       0.80      0.71      0.74      3664\n",
      "weighted avg       0.79      0.79      0.78      3664\n",
      "\n",
      "[[1285  111    0    1]\n",
      " [ 157 1128   27    0]\n",
      " [  28  286  237   24]\n",
      " [   8   40   79  253]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.84      0.85      1397\n",
      "           2       0.71      0.73      0.72      1312\n",
      "           3       0.53      0.56      0.54       575\n",
      "           4       0.67      0.64      0.66       380\n",
      "\n",
      "    accuracy                           0.73      3664\n",
      "   macro avg       0.70      0.69      0.69      3664\n",
      "weighted avg       0.74      0.73      0.74      3664\n",
      "\n",
      "[[1167  210   11    9]\n",
      " [ 155  958  168   31]\n",
      " [  18  157  321   79]\n",
      " [   1   26  108  245]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_pp_clf = Runner(train=popularity_features(pd.concat([data.train.features, data.validation.features]), day=1),\n",
    "                train_y=pd.concat([data.train.y, data.validation.y]),\n",
    "                test=popularity_features(data.test.features, day=1),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.switch_label('is_fake_news_label')\n",
    "data.test.switch_label('is_fake_news_label')\n",
    "data.validation.switch_label('is_fake_news_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2184\n",
      "           1       0.98      0.96      0.97      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2149   35]\n",
      " [  65 1415]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      2184\n",
      "           1       0.98      0.92      0.95      1480\n",
      "\n",
      "    accuracy                           0.96      3664\n",
      "   macro avg       0.96      0.95      0.96      3664\n",
      "weighted avg       0.96      0.96      0.96      3664\n",
      "\n",
      "[[2160   24]\n",
      " [ 122 1358]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2184\n",
      "           1       0.89      0.87      0.88      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.90      0.90      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2026  158]\n",
      " [ 198 1282]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_fn_clf1 = Runner(train=detection_features(data.train.features, day=1),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=1),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2184\n",
      "           1       0.98      0.96      0.97      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2149   35]\n",
      " [  65 1415]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      2184\n",
      "           1       0.99      0.90      0.94      1480\n",
      "\n",
      "    accuracy                           0.95      3664\n",
      "   macro avg       0.96      0.94      0.95      3664\n",
      "weighted avg       0.96      0.95      0.95      3664\n",
      "\n",
      "[[2165   19]\n",
      " [ 150 1330]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      2184\n",
      "           1       0.89      0.85      0.87      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.89      0.89      0.89      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2025  159]\n",
      " [ 222 1258]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_fn_clf2 = Runner(train=detection_features(data.train.features, day=1, pop_predictor=ad1_pp_clf[0]),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=1, pop_predictor=ad1_pp_clf[0]),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validacna mnozina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.74      0.82       325\n",
      "           1       0.85      0.95      0.90       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.88      0.85      0.86       802\n",
      "weighted avg       0.87      0.87      0.87       802\n",
      "\n",
      "[[242  83]\n",
      " [ 22 455]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       325\n",
      "           1       0.89      0.84      0.87       477\n",
      "\n",
      "    accuracy                           0.85       802\n",
      "   macro avg       0.84      0.85      0.84       802\n",
      "weighted avg       0.85      0.85      0.85       802\n",
      "\n",
      "[[278  47]\n",
      " [ 77 400]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71       325\n",
      "           1       0.81      0.78      0.79       477\n",
      "\n",
      "    accuracy                           0.76       802\n",
      "   macro avg       0.75      0.76      0.75       802\n",
      "weighted avg       0.76      0.76      0.76       802\n",
      "\n",
      "[[238  87]\n",
      " [106 371]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_fn_valid_clf1 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=1),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=1),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82       325\n",
      "           1       0.85      0.94      0.89       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.87      0.85      0.86       802\n",
      "weighted avg       0.87      0.87      0.86       802\n",
      "\n",
      "[[244  81]\n",
      " [ 27 450]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       325\n",
      "           1       0.89      0.82      0.86       477\n",
      "\n",
      "    accuracy                           0.84       802\n",
      "   macro avg       0.83      0.84      0.83       802\n",
      "weighted avg       0.84      0.84      0.84       802\n",
      "\n",
      "[[279  46]\n",
      " [ 85 392]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74       325\n",
      "           1       0.82      0.81      0.82       477\n",
      "\n",
      "    accuracy                           0.78       802\n",
      "   macro avg       0.78      0.78      0.78       802\n",
      "weighted avg       0.78      0.78      0.78       802\n",
      "\n",
      "[[241  84]\n",
      " [ 89 388]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_fn_valid_clf2 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=1, pop_predictor=ad1_pp_clf[0]),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=1, pop_predictor=ad1_pp_clf[0]),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       325\n",
      "           1       0.85      0.95      0.90       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.88      0.85      0.86       802\n",
      "weighted avg       0.87      0.87      0.87       802\n",
      "\n",
      "[[245  80]\n",
      " [ 25 452]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81       325\n",
      "           1       0.89      0.84      0.86       477\n",
      "\n",
      "    accuracy                           0.84       802\n",
      "   macro avg       0.84      0.84      0.84       802\n",
      "weighted avg       0.85      0.84      0.84       802\n",
      "\n",
      "[[275  50]\n",
      " [ 77 400]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73       325\n",
      "           1       0.82      0.79      0.81       477\n",
      "\n",
      "    accuracy                           0.77       802\n",
      "   macro avg       0.76      0.77      0.77       802\n",
      "weighted avg       0.78      0.77      0.77       802\n",
      "\n",
      "[[243  82]\n",
      " [100 377]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad1_fn_valid_clf3 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=1,\n",
    "                                                              pop_predictor=FakePredictor(pd.concat([data.train.y_all['fb_popularity_ad_10_label'], data.test.y_all['fb_popularity_ad_10_label']]))),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=1, pop_predictor=FakePredictor(data.validation.y_all['fb_popularity_ad_10_label'])),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situacia: Moment 2 dni po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nastavenie labelu\n",
    "data.train.switch_label('fb_popularity_ad_10_label')\n",
    "data.test.switch_label('fb_popularity_ad_10_label')\n",
    "data.validation.switch_label('fb_popularity_ad_10_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.98      0.95      1397\n",
      "           2       0.88      0.90      0.89      1312\n",
      "           3       0.84      0.77      0.80       575\n",
      "           4       0.95      0.79      0.86       380\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.86      0.88      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[1369   27    1    0]\n",
      " [ 107 1176   24    5]\n",
      " [  12  110  442   11]\n",
      " [   1   16   62  301]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.95      0.93      1397\n",
      "           2       0.83      0.90      0.86      1312\n",
      "           3       0.80      0.69      0.74       575\n",
      "           4       0.96      0.74      0.84       380\n",
      "\n",
      "    accuracy                           0.87      3664\n",
      "   macro avg       0.88      0.82      0.84      3664\n",
      "weighted avg       0.87      0.87      0.87      3664\n",
      "\n",
      "[[1333   63    0    1]\n",
      " [ 106 1175   31    0]\n",
      " [  18  147  398   12]\n",
      " [   3   28   66  283]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.88      0.90      1397\n",
      "           2       0.79      0.82      0.80      1312\n",
      "           3       0.69      0.69      0.69       575\n",
      "           4       0.78      0.79      0.79       380\n",
      "\n",
      "    accuracy                           0.82      3664\n",
      "   macro avg       0.79      0.80      0.79      3664\n",
      "weighted avg       0.82      0.82      0.82      3664\n",
      "\n",
      "[[1225  165    2    5]\n",
      " [  98 1075  114   25]\n",
      " [  10  112  399   54]\n",
      " [   0   13   66  301]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad2_pp_clf = Runner(train=popularity_features(pd.concat([data.train.features, data.validation.features]), day=2),\n",
    "                train_y=pd.concat([data.train.y, data.validation.y]),\n",
    "                test=popularity_features(data.test.features, day=2),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nastavenie labelu\n",
    "\n",
    "data.train.switch_label('is_fake_news_label')\n",
    "data.test.switch_label('is_fake_news_label')\n",
    "data.validation.switch_label('is_fake_news_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2184\n",
      "           1       0.98      0.95      0.97      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2149   35]\n",
      " [  67 1413]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      2184\n",
      "           1       0.98      0.91      0.95      1480\n",
      "\n",
      "    accuracy                           0.96      3664\n",
      "   macro avg       0.96      0.95      0.96      3664\n",
      "weighted avg       0.96      0.96      0.96      3664\n",
      "\n",
      "[[2161   23]\n",
      " [ 129 1351]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2184\n",
      "           1       0.89      0.86      0.88      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.90      0.90      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2025  159]\n",
      " [ 200 1280]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad2_fn_clf1 = Runner(train=detection_features(data.train.features, day=2),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=2),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + predikcia popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2184\n",
      "           1       0.98      0.95      0.97      1480\n",
      "\n",
      "    accuracy                           0.97      3664\n",
      "   macro avg       0.97      0.97      0.97      3664\n",
      "weighted avg       0.97      0.97      0.97      3664\n",
      "\n",
      "[[2149   35]\n",
      " [  67 1413]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      2184\n",
      "           1       0.98      0.91      0.95      1480\n",
      "\n",
      "    accuracy                           0.96      3664\n",
      "   macro avg       0.96      0.95      0.96      3664\n",
      "weighted avg       0.96      0.96      0.96      3664\n",
      "\n",
      "[[2159   25]\n",
      " [ 131 1349]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      2184\n",
      "           1       0.89      0.86      0.88      1480\n",
      "\n",
      "    accuracy                           0.90      3664\n",
      "   macro avg       0.90      0.90      0.90      3664\n",
      "weighted avg       0.90      0.90      0.90      3664\n",
      "\n",
      "[[2024  160]\n",
      " [ 200 1280]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad2_fn_clf2 = Runner(train=detection_features(data.train.features, day=2, pop_predictor=ad2_pp_clf[0]),\n",
    "                train_y=data.train.y,\n",
    "                test=detection_features(data.test.features, day=2, pop_predictor=ad2_pp_clf[0]),\n",
    "                test_y=data.test.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validacna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       325\n",
      "           1       0.85      0.95      0.90       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.88      0.85      0.86       802\n",
      "weighted avg       0.88      0.87      0.87       802\n",
      "\n",
      "[[244  81]\n",
      " [ 23 454]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       325\n",
      "           1       0.88      0.86      0.87       477\n",
      "\n",
      "    accuracy                           0.85       802\n",
      "   macro avg       0.84      0.84      0.84       802\n",
      "weighted avg       0.85      0.85      0.85       802\n",
      "\n",
      "[[269  56]\n",
      " [ 67 410]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74       325\n",
      "           1       0.82      0.83      0.83       477\n",
      "\n",
      "    accuracy                           0.79       802\n",
      "   macro avg       0.79      0.78      0.78       802\n",
      "weighted avg       0.79      0.79      0.79       802\n",
      "\n",
      "[[240  85]\n",
      " [ 81 396]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "ad2_fn_valid_clf1 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=2),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=2),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       325\n",
      "           1       0.85      0.95      0.90       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.88      0.85      0.86       802\n",
      "weighted avg       0.88      0.87      0.87       802\n",
      "\n",
      "[[244  81]\n",
      " [ 23 454]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       325\n",
      "           1       0.90      0.84      0.87       477\n",
      "\n",
      "    accuracy                           0.85       802\n",
      "   macro avg       0.85      0.86      0.85       802\n",
      "weighted avg       0.86      0.85      0.85       802\n",
      "\n",
      "[[282  43]\n",
      " [ 74 403]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       325\n",
      "           1       0.81      0.82      0.82       477\n",
      "\n",
      "    accuracy                           0.78       802\n",
      "   macro avg       0.77      0.77      0.77       802\n",
      "weighted avg       0.78      0.78      0.78       802\n",
      "\n",
      "[[236  89]\n",
      " [ 86 391]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "# validacna + predikcia\n",
    "ad2_fn_valid_clf2 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=2,\n",
    "                                                              pop_predictor=ad2_pp_clf[0]),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=2, pop_predictor=ad2_pp_clf[0]),\n",
    "                test_y=data.validation.y).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                    ] (processing: XGBClassifier) -- 0 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       325\n",
      "           1       0.85      0.95      0.90       477\n",
      "\n",
      "    accuracy                           0.87       802\n",
      "   macro avg       0.88      0.85      0.86       802\n",
      "weighted avg       0.87      0.87      0.87       802\n",
      "\n",
      "[[243  82]\n",
      " [ 24 453]]\n",
      "------------------------------------------------------\n",
      "[==================                                  ] (processing: RandomForestClassifier) -- 1 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.82       325\n",
      "           1       0.90      0.83      0.86       477\n",
      "\n",
      "    accuracy                           0.84       802\n",
      "   macro avg       0.84      0.85      0.84       802\n",
      "weighted avg       0.85      0.84      0.84       802\n",
      "\n",
      "[[281  44]\n",
      " [ 83 394]]\n",
      "------------------------------------------------------\n",
      "[===================================                 ] (processing: DecisionTreeClassifier) -- 2 / 3              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75       325\n",
      "           1       0.83      0.84      0.84       477\n",
      "\n",
      "    accuracy                           0.80       802\n",
      "   macro avg       0.80      0.79      0.79       802\n",
      "weighted avg       0.80      0.80      0.80       802\n",
      "\n",
      "[[240  85]\n",
      " [ 74 403]]\n",
      "------------------------------------------------------\n",
      "[====================================================] -- 3 / 3 -- (finished)\n"
     ]
    }
   ],
   "source": [
    "# validacna + real\n",
    "ad2_fn_valid_clf3 = Runner(train=detection_features(pd.concat([data.train.features, data.test.features]), day=2,\n",
    "                                                              pop_predictor=FakePredictor(pd.concat([data.train.y_all['fb_popularity_ad_10_label'], data.test.y_all['fb_popularity_ad_10_label']]))),\n",
    "                train_y=pd.concat([data.train.y, data.test.y]),\n",
    "                test=detection_features(data.validation.features, day=2, pop_predictor=FakePredictor(data.validation.y_all['fb_popularity_ad_10_label'])),\n",
    "                test_y=data.validation.y).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
