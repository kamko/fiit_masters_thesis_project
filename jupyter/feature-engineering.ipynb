{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import logging\n",
    "import empath\n",
    "\n",
    "import re\n",
    "import common\n",
    "import util\n",
    "importlib.reload(common)\n",
    "importlib.reload(util)\n",
    "\n",
    "from common import create_engine\n",
    "from common import display_all\n",
    "from common import figsize\n",
    "from common import save_df, load_df\n",
    "from common import save_session, load_session\n",
    "\n",
    "from util import show_importances\n",
    "from util import split_X_y_all, split_X_y, split_data\n",
    "from util import empty_features, column_feature, str_contains\n",
    "\n",
    "from pbar import Pbar\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters() # converters e.g. for datetime in plots\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import textstat\n",
    "\n",
    "import spacy\n",
    "importlib.reload(spacy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text, disable=['parser', 'tagger', 'ner'])\n",
    "    \n",
    "    return words_from_doc(doc)\n",
    "\n",
    "def words_from_doc(doc): \n",
    "    res = []\n",
    "    for i in doc:\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.is_punct:\n",
    "            continue\n",
    "            \n",
    "        res.append(str(i))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def doc_tokens_to_file(data, file):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for i in Pbar(data):\n",
    "            f.write(f\"{' '.join(words_from_doc(i))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15458\n"
     ]
    }
   ],
   "source": [
    "df = load_df('final_data.pickle')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_body = list(nlp.pipe(df.body))\n",
    "spacy_title = list(nlp.pipe(df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblob_body = [TextBlob(i) for i in df.body]\n",
    "tblob_title = [TextBlob(i) for i in df.title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "\n",
    "    body_sentiment = [i.sentiment for i in tblob_body]\n",
    "    res['sentiment_body_subjectivity'] = [i.subjectivity for i in body_sentiment]\n",
    "    res['sentiment_body_polarity'] = [i.polarity for i in body_sentiment]\n",
    "    \n",
    "    \n",
    "    title_sentiment = [i.sentiment for i in tblob_title]\n",
    "    res['sentiment_title_subjectivity'] = [i.subjectivity for i in title_sentiment]\n",
    "    res['sentiment_title_polarity'] = [i.polarity for i in title_sentiment]\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(sentiment_features(), 'features_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['readability_body_ari'] = df.body.apply(textstat.automated_readability_index)\n",
    "    res['readability_body_fcgl'] = df.body.apply(textstat.flesch_kincaid_grade)\n",
    "    res['readability_body_frei'] = df.body.apply(textstat.flesch_reading_ease)\n",
    "    res['readability_body_gfi'] = df.body.apply(textstat.gunning_fog)\n",
    "    res['readability_body_cli'] = df.body.apply(textstat.coleman_liau_index)\n",
    "    \n",
    "    res['readability_title_ari'] = df.title.apply(textstat.automated_readability_index)\n",
    "    res['readability_title_fcgl'] = df.title.apply(textstat.flesch_kincaid_grade)\n",
    "    res['readability_title_frei'] = df.title.apply(textstat.flesch_reading_ease)\n",
    "    res['readability_title_gfi'] = df.title.apply(textstat.gunning_fog)\n",
    "    res['readability_title_cli'] = df.title.apply(textstat.coleman_liau_index)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(readability_features(), 'features_readability.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 10:32:52,605 : INFO : Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-05-06 10:32:52,606 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "def metadata_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['metadata_published_at_day'] = df.published_at.dt.weekday + 1\n",
    "    \n",
    "    dummies = pd.get_dummies(df.source_name)\n",
    "    dummies.columns = [f'source_{col}' for col in dummies.columns]\n",
    "    \n",
    "    res = pd.concat([res, dummies], axis=1, join='inner')\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(metadata_features(), 'features_metadata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empath_features():\n",
    "    analyzer = empath.Empath()\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for i, doc in enumerate(spacy_body):\n",
    "        lemmatized_doc = ' '.join([token.lemma_ for token in doc])\n",
    "        analyzed = analyzer.analyze(lemmatized_doc, normalize=True) or {}\n",
    "        em_vals = {f'empath_{k}': v for k,v in analyzed.items()}\n",
    "        res.append(em_vals)\n",
    "    \n",
    "    return pd.DataFrame(res, index=df.index).fillna(0)\n",
    "\n",
    "save_df(empath_features(), 'features_empath.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "        \n",
    "    res['content_title_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_title]\n",
    "    \n",
    "    \n",
    "    res['content_body_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "       \n",
    "    res['content_avg_word_len'] = [np.mean([len(t.text) for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_sentence_count'] = [len(list(doc.sents)) for doc in spacy_body]\n",
    "    \n",
    "    res['content_word_over_5ch'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False and len(t.text) > 5])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_stop_words_count'] = [len([t for t in doc if t.is_stop]) for doc in spacy_body]\n",
    "    \n",
    "    res['content_body_len'] = df.body.apply(lambda x: len(x))\n",
    "    res['content_title_len'] = df.title.apply(lambda x: len(x))\n",
    "    res['content_?_count'] = df.body.apply(lambda x: x.count('?'))\n",
    "    res['content_!_count'] = df.body.apply(lambda x: x.count('!'))\n",
    "    res['content_..._count'] = df.body.apply(lambda x: x.count('...'))\n",
    "    \n",
    "    res['content_media_count'] = df['image_count'] + df['video_count'] + df.body_urls.apply(lambda x: len([i for i in x if re.search(r'\\.jpg|\\.jpeg|\\.png|\\.bmp', i)]))\n",
    "    \n",
    "    pos_tags = [Counter(\n",
    "                        [t.tag_ for t in doc if t.is_punct is False])for doc in spacy_body]\n",
    "    pos_tags_df = pd.DataFrame(pos_tags,\n",
    "                               columns=nlp.get_pipe('tagger').labels,\n",
    "                               index=df.index).fillna(0)\n",
    "    pos_tags_df.columns = [f'content_pos_{c}' for c in pos_tags_df.columns]\n",
    "    \n",
    "    res = pd.concat([res, pos_tags_df], axis=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(content_features(), 'features_content.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_features():\n",
    "    \n",
    "    ners_1 = [Counter([t.label_ for t in doc.ents]) for doc in spacy_body]\n",
    "    res_1 = pd.DataFrame(ners_1, index=df.index, columns=nlp.get_pipe(\"ner\").labels).fillna(0)\n",
    "    res_1.columns = [f'named_entity_body_{c}' for c in res_1.columns]\n",
    "    \n",
    "    ners_2 = [Counter([t.label_ for t in doc.ents]) for doc in spacy_title]\n",
    "    res_2 = pd.DataFrame(ners_2, index=df.index, columns=nlp.get_pipe(\"ner\").labels).fillna(0)\n",
    "    res_2.columns = [f'named_entity_title_{c}' for c in res_2.columns]\n",
    "    \n",
    "    return pd.concat([res_1, res_2], axis=1)\n",
    "\n",
    "save_df(named_entities_features(), 'features_named_entities.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_features(day):\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res[f'fb_ad_{day}_reaction_count'] = df[f'fb_ad_{day}_reaction_count']\n",
    "    res[f'fb_ad_{day}_comment_count'] = df[f'fb_ad_{day}_comment_count']\n",
    "    res[f'fb_ad_{day}_share_count'] = df[f'fb_ad_{day}_share_count']\n",
    "    res[f'fb_popularity_ad_{day}'] = df[f'fb_popularity_ad_{day}']\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(popularity_features(0), 'features_popularity_0.pickle')\n",
    "save_df(popularity_features(1), 'features_popularity_1.pickle')\n",
    "save_df(popularity_features(2), 'features_popularity_2.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
