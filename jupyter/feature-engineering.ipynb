{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import logging\n",
    "import empath\n",
    "\n",
    "import common\n",
    "import util\n",
    "importlib.reload(common)\n",
    "importlib.reload(util)\n",
    "\n",
    "from common import create_engine\n",
    "from common import display_all\n",
    "from common import figsize\n",
    "from common import save_df, load_df\n",
    "from common import save_session, load_session\n",
    "\n",
    "from util import show_importances\n",
    "from util import split_X_y_all, split_X_y, split_data\n",
    "from util import empty_features, column_feature, str_contains\n",
    "\n",
    "from pbar import Pbar\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters() # converters e.g. for datetime in plots\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import textstat\n",
    "\n",
    "import spacy\n",
    "importlib.reload(spacy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text, disable=['parser', 'tagger', 'ner'])\n",
    "    \n",
    "    return words_from_doc(doc)\n",
    "\n",
    "def words_from_doc(doc): \n",
    "    res = []\n",
    "    for i in doc:\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.is_punct:\n",
    "            continue\n",
    "            \n",
    "        res.append(str(i))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def doc_tokens_to_file(data, file):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for i in Pbar(data):\n",
    "            f.write(f\"{' '.join(words_from_doc(i))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20246\n"
     ]
    }
   ],
   "source": [
    "df = load_df('final_data.pickle')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_features = pd.DataFrame(index=df.index)\n",
    "tfidf_features = pd.DataFrame(index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_body = list(nlp.pipe(df.body))\n",
    "spacy_title = list(nlp.pipe(df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblob_body = [TextBlob(i) for i in df.body]\n",
    "tblob_title = [TextBlob(i) for i in df.title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "\n",
    "    body_sentiment = [i.sentiment for i in tblob_body]\n",
    "    res['sentiment_body_subjectivity'] = [i.subjectivity for i in body_sentiment]\n",
    "    res['sentiment_body_polarity'] = [i.polarity for i in body_sentiment]\n",
    "    \n",
    "    \n",
    "    title_sentiment = [i.sentiment for i in tblob_title]\n",
    "    res['sentiment_title_subjectivity'] = [i.subjectivity for i in title_sentiment]\n",
    "    res['sentiment_title_polarity'] = [i.polarity for i in title_sentiment]\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(sentiment_features(), 'features_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['readability_ari'] = df.body.apply(textstat.automated_readability_index)\n",
    "    res['readability_fcgl'] = df.body.apply(textstat.flesch_kincaid_grade)\n",
    "    res['readability_frei'] = df.body.apply(textstat.flesch_reading_ease)\n",
    "    res['readability_gfi'] = df.body.apply(textstat.gunning_fog)\n",
    "    res['readability_cli'] = df.body.apply(textstat.coleman_liau_index)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(readability_features(), 'features_readability.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-04 21:58:45,752 : INFO : Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-05-04 21:58:45,753 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "def metadata_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['metadata_published_at_day'] = df.published_at.dt.weekday + 1\n",
    "    res['metadata_source_name'] = df.source_name\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(metadata_features(), 'features_metadata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empath_features():\n",
    "    analyzer = empath.Empath()\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for doc in spacy_body:\n",
    "        lemmatized_doc = ' '.join([token.lemma_ for token in doc])\n",
    "        em_vals = {f'empath_{k}': v for k,v in analyzer.analyze(lemmatized_doc).items()}\n",
    "        res.append(em_vals)\n",
    "    \n",
    "    return pd.DataFrame(res, index=df.index)\n",
    "\n",
    "save_df(empath_features(), 'features_empath.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "        \n",
    "    res['content_title_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_title]\n",
    "    \n",
    "    \n",
    "    res['content_body_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "       \n",
    "    res['content_avg_word_len'] = [np.mean([len(t.text) for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_sentence_count'] = [len(list(doc.sents)) for doc in spacy_body]\n",
    "    \n",
    "    res['content_word_over_5ch'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False and len(t.text) > 5])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_stop_words_count'] = [len([t for t in doc if t.is_stop]) for doc in spacy_body]\n",
    "    \n",
    "    res['content_body_len'] = df.body.apply(lambda x: len(x))\n",
    "    res['content_title_len'] = df.title.apply(lambda x: len(x))\n",
    "    res['content_?_count'] = df.body.apply(lambda x: x.count('?'))\n",
    "    res['content_!_count'] = df.body.apply(lambda x: x.count('!'))\n",
    "    res['content_..._count'] = df.body.apply(lambda x: x.count('...'))\n",
    "    \n",
    "    res['content_media_count'] = df['image_count'] + df['video_count']\n",
    "    \n",
    "    pos_tags = [Counter([t.tag_ for t in doc if t.is_punct is False]) for doc in spacy_body]\n",
    "    pos_tags_df = pd.DataFrame(pos_tags, columns=nlp.get_pipe('tagger').labels, index=df.index).fillna(0)\n",
    "    pos_tags_df.columns = [f'content_pos_{c}' for c in pos_tags_df.columns]\n",
    "    \n",
    "    res = pd.concat([res, pos_tags_df], axis=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(content_features(), 'features_content.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_features():\n",
    "    \n",
    "    ners = [Counter([t.label_ for t in doc.ents]) for doc in spacy_body]\n",
    "    res = pd.DataFrame(ners, index=df.index, columns=nlp.get_pipe(\"ner\").labels).fillna(0)\n",
    "    res.columns = [f'named_entity_{c}' for c in res.columns]\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(named_entities_features(), 'features_named_entities.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for i in [0,1,3]:\n",
    "        res[f'fb_ad_{i}_reaction_count'] = df[f'fb_ad_{i}_reaction_count']\n",
    "        res[f'fb_ad_{i}_comment_count'] = df[f'fb_ad_{i}_comment_count']\n",
    "        res[f'fb_ad_{i}_share_count'] = df[f'fb_ad_{i}_share_count']\n",
    "        res[f'fb_popularity_ad_{i}'] = df[f'fb_popularity_ad_{i}']\n",
    "        \n",
    "    \n",
    "    res.fillna(res.mean(), inplace=True)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(popularity_features(), 'features_popularity.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
