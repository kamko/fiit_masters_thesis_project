{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import logging\n",
    "import empath\n",
    "\n",
    "import common\n",
    "import util\n",
    "importlib.reload(common)\n",
    "importlib.reload(util)\n",
    "\n",
    "from common import create_engine\n",
    "from common import display_all\n",
    "from common import figsize\n",
    "from common import save_df, load_df\n",
    "from common import save_session, load_session\n",
    "\n",
    "from util import show_importances\n",
    "from util import split_X_y_all, split_X_y, split_data\n",
    "from util import empty_features, column_feature, str_contains\n",
    "\n",
    "from pbar import Pbar\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters() # converters e.g. for datetime in plots\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prefer_gpu' from 'thinc.neural.util' (C:\\Users\\kamko\\Anaconda3\\lib\\site-packages\\thinc\\neural\\util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-3f740924742a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"spec not found for the module {name!r}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;31m# The module may have replaced itself in sys.modules!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_exec\u001b[1;34m(spec, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# These are imported as part of the API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'prefer_gpu' from 'thinc.neural.util' (C:\\Users\\kamko\\Anaconda3\\lib\\site-packages\\thinc\\neural\\util.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import textstat\n",
    "\n",
    "import spacy\n",
    "importlib.reload(spacy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20279\n"
     ]
    }
   ],
   "source": [
    "df = load_df('final_data.pickle')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_features = pd.DataFrame(index=df.index)\n",
    "tfidf_features = pd.DataFrame(index=df.index)\n",
    "popularity_features = pd.DataFrame(index=df.index)\n",
    "named_entities_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_body = list(nlp.pipe(df.body))\n",
    "spacy_title = list(nlp.pipe(df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblob_body = [TextBlob(i) for i in df.body]\n",
    "tblob_title = [TextBlob(i) for i in df.title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "\n",
    "    body_sentiment = [i.sentiment for i in tblob_body]\n",
    "    res['sentiment_body_subjectivity'] = [i.subjectivity for i in body_sentiment]\n",
    "    res['sentiment_body_polarity'] = [i.polarity for i in body_sentiment]\n",
    "    \n",
    "    \n",
    "    title_sentiment = [i.sentiment for i in tblob_title]\n",
    "    res['sentiment_title_subjectivity'] = [i.subjectivity for i in title_sentiment]\n",
    "    res['sentiment_title_polarity'] = [i.polarity for i in title_sentiment]\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(sentiment_features(), 'features_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['readability_ari'] = df.body.apply(textstat.automated_readability_index)\n",
    "    res['readability_fcgl'] = df.body.apply(textstat.flesch_kincaid_grade)\n",
    "    res['readability_frei'] = df.body.apply(textstat.flesch_reading_ease)\n",
    "    res['readability_gfi'] = df.body.apply(textstat.gunning_fog)\n",
    "    res['readability_cli'] = df.body.apply(textstat.coleman_liau_index)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(readability_features(), 'features_readability.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-04 19:13:56,791 : INFO : Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-05-04 19:13:56,792 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "def metadata_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    res['metadata_published_at_day'] = df.published_at.dt.weekday + 1\n",
    "    res['metadata_source_name'] = df.source_name\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(metadata_features(), 'features_metadata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empath_features():\n",
    "    analyzer = empath.Empath()\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for doc in spacy_body:\n",
    "        lemmatized_doc = ' '.join([token.lemma_ for token in doc])\n",
    "        em_vals = {f'empath_{k}': v for k,v in analyzer.analyze(lemmatized_doc).items()}\n",
    "        res.append(em_vals)\n",
    "    \n",
    "    return pd.DataFrame(res, index=df.index)\n",
    "\n",
    "save_df(empath_features(), 'features_empath.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_features():\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "        \n",
    "    res['content_title_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_title]\n",
    "    \n",
    "    \n",
    "    res['content_body_word_count'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "       \n",
    "    res['content_avg_word_len'] = [np.mean([len(t.text) for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_sentence_count'] = [len(list(doc.sents)) for doc in spacy_body]\n",
    "    \n",
    "    res['content_word_over_5ch'] = [len([t for t in doc \n",
    "                                         if t.is_punct is False and t.is_stop is False and len(t.text) > 5])\n",
    "                                    for doc in spacy_body]\n",
    "    \n",
    "    res['content_stop_words_count'] = [len([t for t in doc if t.is_stop]) for doc in spacy_body]\n",
    "    \n",
    "    res['content_body_len'] = df.body.apply(lambda x: len(x))\n",
    "    res['content_title_len'] = df.title.apply(lambda x: len(x))\n",
    "    res['content_?_count'] = df.body.apply(lambda x: x.count('?'))\n",
    "    res['content_!_count'] = df.body.apply(lambda x: x.count('!'))\n",
    "    res['content_..._count'] = df.body.apply(lambda x: x.count('...'))\n",
    "    \n",
    "    res['content_media_count'] = df['image_count'] + df['video_count']\n",
    "    \n",
    "    pos_tags = [Counter([t.tag_ for t in doc if t.is_punct is False]) for doc in spacy_body]\n",
    "    pos_tags_df = pd.DataFrame(pos_tags, columns=nlp.get_pipe('tagger').labels, index=df.index).fillna(0)\n",
    "    pos_tags_df.columns = [f'content_pos_{c}' for c in pos_tags_df.columns]\n",
    "    \n",
    "    res = pd.concat([res, pos_tags_df], axis=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "save_df(content_features(), 'features_content.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_features():\n",
    "    \n",
    "    ners = [Counter([t.label_ for t in doc.ents]) for doc in spacy_body]\n",
    "    res = pd.DataFrame(ners, index=df.index. columns=nlp.get_pipe(\"ner\").labels).fillna(0)\n",
    "    res.columns = [f'named_entity_{c}' for c in res.columns]\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thousands of years DATE\n",
      "first ORDINAL\n",
      "thousands of years DATE\n",
      "around 1000 CARDINAL\n",
      "the past few decades DATE\n",
      "2 CARDINAL\n",
      "one CARDINAL\n",
      "one CARDINAL\n",
      "first ORDINAL\n",
      "daily DATE\n",
      "daily DATE\n",
      "days DATE\n",
      "4 CARDINAL\n",
      "first ORDINAL\n",
      "over 1000 CARDINAL\n",
      "only one CARDINAL\n",
      "two CARDINAL\n",
      "75% PERCENT\n",
      "second ORDINAL\n",
      "another fall season DATE\n",
      "first ORDINAL\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.pipeline.EntityRecognizer' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-5e93f0166d0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.pipeline.EntityRecognizer' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "for i in spacy_body[0].ents:\n",
    "    print(i.text, i.label_)\n",
    "    \n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
